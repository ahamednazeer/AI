{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMr7I+hcxLgv8QwdabFcXEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahamednazeer/AI/blob/master/6G_technologies_for_healthcare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tQ9SZIFkgpTc",
        "outputId": "d6d41863-3893-4715-ef56-e821637ee318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.14)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Please upload kaggle.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8a38d2dd-3c8b-4f79-9828-651bf2c5cc2c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8a38d2dd-3c8b-4f79-9828-651bf2c5cc2c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n",
            "Uploaded file: kaggle (2).json\n",
            "Dataset URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
            "License(s): other\n",
            "chest-xray-pneumonia.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace /content/chest_xray/chest_xray/__MACOSX/._chest_xray? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "Creating small dataset...\n",
            "Source directory: /content/chest_xray/chest_xray\n",
            "Found 1341 NORMAL training images\n",
            "Found 3875 PNEUMONIA training images\n",
            "Found 234 NORMAL test images\n",
            "Found 390 PNEUMONIA test images\n",
            "Copied 250 NORMAL training images\n",
            "Copied 250 PNEUMONIA training images\n",
            "Copied 50 NORMAL test images\n",
            "Copied 50 PNEUMONIA test images\n",
            "Loading small Pneumonia dataset...\n",
            "Found 500 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n",
            "\n",
            "Simulating 6G network...\n",
            "\n",
            "Federated Learning Round 1/3 (6G)\n",
            "Training on IoMT Device 1\n",
            "Data transmission time (6G): 0.7635 seconds\n",
            "Local training time on Device 1: 17.1413 seconds\n",
            "Training on IoMT Device 2\n",
            "Data transmission time (6G): 0.7627 seconds\n",
            "Local training time on Device 2: 6.2544 seconds\n",
            "Training on IoMT Device 3\n",
            "Data transmission time (6G): 0.7631 seconds\n",
            "Local training time on Device 3: 4.3782 seconds\n",
            "\n",
            "Federated Learning Round 2/3 (6G)\n",
            "Training on IoMT Device 1\n",
            "Data transmission time (6G): 0.7631 seconds\n",
            "Local training time on Device 1: 5.6884 seconds\n",
            "Training on IoMT Device 2\n",
            "Data transmission time (6G): 0.7634 seconds\n",
            "Local training time on Device 2: 6.0216 seconds\n",
            "Training on IoMT Device 3\n",
            "Data transmission time (6G): 0.7634 seconds\n",
            "Local training time on Device 3: 5.7482 seconds\n",
            "\n",
            "Federated Learning Round 3/3 (6G)\n",
            "Training on IoMT Device 1\n",
            "Data transmission time (6G): 0.7631 seconds\n",
            "Local training time on Device 1: 5.7504 seconds\n",
            "Training on IoMT Device 2\n",
            "Data transmission time (6G): 0.7627 seconds\n",
            "Local training time on Device 2: 4.8012 seconds\n",
            "Training on IoMT Device 3\n",
            "Data transmission time (6G): 0.7633 seconds\n",
            "Local training time on Device 3: 5.3035 seconds\n",
            "6G - Test Accuracy: 77.00%, Test Loss: 0.4126, Avg Transmission Time: 0.7631 seconds, Total Time: 63.7416 seconds\n",
            "\n",
            "Simulating 5G network...\n",
            "\n",
            "Federated Learning Round 1/3 (5G)\n",
            "Training on IoMT Device 1\n",
            "Data transmission time (5G): 7.6322 seconds\n",
            "Local training time on Device 1: 17.3008 seconds\n",
            "Training on IoMT Device 2\n",
            "Data transmission time (5G): 7.6322 seconds\n",
            "Local training time on Device 2: 5.7301 seconds\n",
            "Training on IoMT Device 3\n",
            "Data transmission time (5G): 7.6344 seconds\n",
            "Local training time on Device 3: 10.8294 seconds\n",
            "\n",
            "Federated Learning Round 2/3 (5G)\n",
            "Training on IoMT Device 1\n",
            "Data transmission time (5G): 7.6301 seconds\n",
            "Local training time on Device 1: 5.8184 seconds\n",
            "Training on IoMT Device 2\n",
            "Data transmission time (5G): 7.6302 seconds\n",
            "Local training time on Device 2: 4.3042 seconds\n",
            "Training on IoMT Device 3\n",
            "Data transmission time (5G): 7.6271 seconds\n",
            "Local training time on Device 3: 5.7402 seconds\n",
            "\n",
            "Federated Learning Round 3/3 (5G)\n",
            "Training on IoMT Device 1\n",
            "Data transmission time (5G): 7.6288 seconds\n",
            "Local training time on Device 1: 5.6654 seconds\n",
            "Training on IoMT Device 2\n",
            "Data transmission time (5G): 7.6334 seconds\n",
            "Local training time on Device 2: 4.0369 seconds\n",
            "Training on IoMT Device 3\n",
            "Data transmission time (5G): 7.6281 seconds\n",
            "Local training time on Device 3: 5.0897 seconds\n",
            "5G - Test Accuracy: 76.00%, Test Loss: 0.4162, Avg Transmission Time: 7.6307 seconds, Total Time: 67.1407 seconds\n",
            "\n",
            "Starting Flask app...\n",
            "Please enter your Ngrok authtoken (from https://dashboard.ngrok.com/get-started/your-authtoken):\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Address already in use\n",
            "Port 5000 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2YsmHMOy8PrZC6e17Q0XxAVdvTt_6iSPDiqyJfh2Uu4hZtvqb\n",
            "Dashboard is publicly accessible at: https://e1f08b4b0482.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Set up the environment in Colab\n",
        "!pip install kaggle tensorflow numpy flask pyngrok\n",
        "\n",
        "# Step 2: Check if Google Drive is mounted\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Drive already mounted or error occurred. Proceeding...\")\n",
        "\n",
        "# Step 3: Set up Kaggle API\n",
        "from google.colab import files\n",
        "print(\"Please upload kaggle.json\")\n",
        "uploaded = files.upload()\n",
        "kaggle_file = next(iter(uploaded))\n",
        "print(f\"Uploaded file: {kaggle_file}\")\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp \"{kaggle_file}\" ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Step 4: Download the Chest X-ray Pneumonia dataset\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "!unzip -q chest-xray-pneumonia.zip -d /content/chest_xray\n",
        "\n",
        "# Step 5: Import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "from flask import Flask, render_template_string\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "\n",
        "# Step 6: Create a small dataset subset\n",
        "def create_small_dataset(source_dir, target_dir, num_train=500, num_test=100):\n",
        "    print(f\"Source directory: {source_dir}\")\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(target_dir, 'train/NORMAL'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(target_dir, 'train/PNEUMONIA'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(target_dir, 'test/NORMAL'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(target_dir, 'test/PNEUMONIA'), exist_ok=True)\n",
        "\n",
        "    normal_train = glob(os.path.join(source_dir, 'train/NORMAL/*.jpeg'))\n",
        "    pneumonia_train = glob(os.path.join(source_dir, 'train/PNEUMONIA/*.jpeg'))\n",
        "    normal_test = glob(os.path.join(source_dir, 'test/NORMAL/*.jpeg'))\n",
        "    pneumonia_test = glob(os.path.join(source_dir, 'test/PNEUMONIA/*.jpeg'))\n",
        "\n",
        "    print(f\"Found {len(normal_train)} NORMAL training images\")\n",
        "    print(f\"Found {len(pneumonia_train)} PNEUMONIA training images\")\n",
        "    print(f\"Found {len(normal_test)} NORMAL test images\")\n",
        "    print(f\"Found {len(pneumonia_test)} PNEUMONIA test images\")\n",
        "\n",
        "    normal_train = normal_train[:min(num_train//2, len(normal_train))]\n",
        "    pneumonia_train = pneumonia_train[:min(num_train//2, len(pneumonia_train))]\n",
        "    normal_test = normal_test[:min(num_test//2, len(normal_test))]\n",
        "    pneumonia_test = pneumonia_test[:min(num_test//2, len(pneumonia_test))]\n",
        "\n",
        "    for img in normal_train:\n",
        "        shutil.copy(img, os.path.join(target_dir, 'train/NORMAL'))\n",
        "    for img in pneumonia_train:\n",
        "        shutil.copy(img, os.path.join(target_dir, 'train/PNEUMONIA'))\n",
        "    for img in normal_test:\n",
        "        shutil.copy(img, os.path.join(target_dir, 'test/NORMAL'))\n",
        "    for img in pneumonia_test:\n",
        "        shutil.copy(img, os.path.join(target_dir, 'test/PNEUMONIA'))\n",
        "\n",
        "    print(f\"Copied {len(glob(os.path.join(target_dir, 'train/NORMAL/*.jpeg')))} NORMAL training images\")\n",
        "    print(f\"Copied {len(glob(os.path.join(target_dir, 'train/PNEUMONIA/*.jpeg')))} PNEUMONIA training images\")\n",
        "    print(f\"Copied {len(glob(os.path.join(target_dir, 'test/NORMAL/*.jpeg')))} NORMAL test images\")\n",
        "    print(f\"Copied {len(glob(os.path.join(target_dir, 'test/PNEUMONIA/*.jpeg')))} PNEUMONIA test images\")\n",
        "\n",
        "# Step 7: Simulate 5G and 6G network characteristics\n",
        "class Network:\n",
        "    def __init__(self, network_type=\"6G\"):\n",
        "        if network_type == \"6G\":\n",
        "            self.latency_ms = random.uniform(0.1, 1.0)  # 6G: 0.1–1 ms\n",
        "            self.bandwidth_gbps = 1.0  # 6G: 1 Gbps\n",
        "            self.reliability = 0.99999  # 6G: 99.999%\n",
        "        else:  # 5G\n",
        "            self.latency_ms = random.uniform(1.0, 10.0)  # 5G: 1–10 ms\n",
        "            self.bandwidth_gbps = 0.1  # 5G: 100 Mbps\n",
        "            self.reliability = 0.999  # 5G: 99.9%\n",
        "        self.network_type = network_type\n",
        "\n",
        "    def transmit_data(self, data_size_mb):\n",
        "        transmission_time = (data_size_mb * 8) / self.bandwidth_gbps / 1000\n",
        "        transmission_time += self.latency_ms / 1000\n",
        "        if random.random() > self.reliability:\n",
        "            raise Exception(f\"Transmission failed due to {self.network_type} network error!\")\n",
        "        return transmission_time\n",
        "\n",
        "# Step 8: Load the small dataset\n",
        "def load_small_dataset(data_dir, img_size=(224, 224), batch_size=16):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        zoom_range=0.1\n",
        "    )\n",
        "\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        os.path.join(data_dir, 'train'),\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        classes=['NORMAL', 'PNEUMONIA'],\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_generator = datagen.flow_from_directory(\n",
        "        os.path.join(data_dir, 'test'),\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        classes=['NORMAL', 'PNEUMONIA'],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    if train_generator.samples == 0 or test_generator.samples == 0:\n",
        "        raise ValueError(f\"No images found in the dataset. Check directory: {data_dir}\")\n",
        "\n",
        "    return train_generator, test_generator\n",
        "\n",
        "# Step 9: Build transfer learning model\n",
        "def build_transfer_learning_model():\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Step 10: Simulate federated learning with 5G/6G\n",
        "def federated_learning_simulation(model, train_generator, num_devices=3, num_rounds=3, epochs_per_round=1, network_type=\"6G\"):\n",
        "    global_weights = model.get_weights()\n",
        "    batch_size = train_generator.batch_size\n",
        "    total_samples = train_generator.samples\n",
        "    samples_per_device = total_samples // num_devices\n",
        "    transmission_times = []\n",
        "\n",
        "    for round in range(num_rounds):\n",
        "        print(f\"\\nFederated Learning Round {round + 1}/{num_rounds} ({network_type})\")\n",
        "        local_models = []\n",
        "\n",
        "        for device_id in range(num_devices):\n",
        "            print(f\"Training on IoMT Device {device_id + 1}\")\n",
        "            model.set_weights(global_weights)\n",
        "            network = Network(network_type)\n",
        "            data_size_mb = (samples_per_device * 224 * 224 * 3 * 32) / (8 * 1024 * 1024)\n",
        "            transmission_time = network.transmit_data(data_size_mb)\n",
        "            transmission_times.append(transmission_time)\n",
        "            print(f\"Data transmission time ({network_type}): {transmission_time:.4f} seconds\")\n",
        "\n",
        "            start_time = time.time()\n",
        "            steps_per_epoch = max(1, samples_per_device // batch_size)\n",
        "            model.fit(train_generator, epochs=epochs_per_round, steps_per_epoch=steps_per_epoch, verbose=0)\n",
        "            print(f\"Local training time on Device {device_id + 1}: {time.time() - start_time:.4f} seconds\")\n",
        "            local_models.append(model.get_weights())\n",
        "\n",
        "        global_weights = [np.mean([local_weights[i] for local_weights in local_models], axis=0)\n",
        "                         for i in range(len(global_weights))]\n",
        "        model.set_weights(global_weights)\n",
        "\n",
        "    return model, transmission_times\n",
        "\n",
        "# Step 11: Flask app for dashboard\n",
        "app = Flask(__name__)\n",
        "results = {}\n",
        "\n",
        "@app.route('/')\n",
        "def dashboard():\n",
        "    html = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Pneumonia Detection: 6G vs 5G Simulation</title>\n",
        "        <style>\n",
        "            body { font-family: Arial, sans-serif; margin: 20px; background-color: #f0f0f0; }\n",
        "            h1 { color: #333; }\n",
        "            table { border-collapse: collapse; width: 80%; margin: 20px auto; }\n",
        "            th, td { border: 1px solid #ccc; padding: 10px; text-align: center; }\n",
        "            th { background-color: #4CAF50; color: white; }\n",
        "            tr:nth-child(even) { background-color: #f2f2f2; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Pneumonia Detection: 6G vs 5G Simulation</h1>\n",
        "        <table>\n",
        "            <tr>\n",
        "                <th>Network</th>\n",
        "                <th>Test Accuracy (%)</th>\n",
        "                <th>Test Loss</th>\n",
        "                <th>Avg Transmission Time (s)</th>\n",
        "            </tr>\n",
        "            {% for network, data in results.items() %}\n",
        "            <tr>\n",
        "                <td>{{ network }}</td>\n",
        "                <td>{{ data.accuracy }}</td>\n",
        "                <td>{{ data.loss }}</td>\n",
        "                <td>{{ data.avg_transmission_time }}</td>\n",
        "            </tr>\n",
        "            {% endfor %}\n",
        "        </table>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return render_template_string(html, results=results)\n",
        "\n",
        "# Step 12: Main simulation\n",
        "def main():\n",
        "    global results\n",
        "\n",
        "    # Create small dataset\n",
        "    print(\"Creating small dataset...\")\n",
        "    source_dir = '/content/chest_xray/chest_xray'\n",
        "    target_dir = '/content/small_chest_xray'\n",
        "    create_small_dataset(source_dir, target_dir, num_train=500, num_test=100)\n",
        "\n",
        "    # Load the small dataset\n",
        "    print(\"Loading small Pneumonia dataset...\")\n",
        "    try:\n",
        "        train_generator, test_generator = load_small_dataset(target_dir, batch_size=16)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return\n",
        "\n",
        "    # Run simulations for 6G and 5G\n",
        "    for network_type in [\"6G\", \"5G\"]:\n",
        "        print(f\"\\nSimulating {network_type} network...\")\n",
        "        model = build_transfer_learning_model()\n",
        "        start_time = time.time()\n",
        "        model, transmission_times = federated_learning_simulation(\n",
        "            model, train_generator, num_devices=3, num_rounds=3, network_type=network_type\n",
        "        )\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        # Evaluate the model\n",
        "        loss, accuracy = model.evaluate(test_generator, verbose=0)\n",
        "        avg_transmission_time = np.mean(transmission_times)\n",
        "\n",
        "        results[network_type] = {\n",
        "            'accuracy': f\"{accuracy * 100:.2f}\",\n",
        "            'loss': f\"{loss:.4f}\",\n",
        "            'avg_transmission_time': f\"{avg_transmission_time:.4f}\"\n",
        "        }\n",
        "        print(f\"{network_type} - Test Accuracy: {accuracy * 100:.2f}%, Test Loss: {loss:.4f}, \"\n",
        "              f\"Avg Transmission Time: {avg_transmission_time:.4f} seconds, Total Time: {total_time:.4f} seconds\")\n",
        "\n",
        "    # Start Flask app in a separate thread\n",
        "    print(\"\\nStarting Flask app...\")\n",
        "    threading.Thread(target=lambda: app.run(host='0.0.0.0', port=5000)).start()\n",
        "\n",
        "    # Set up Ngrok\n",
        "    print(\"Please enter your Ngrok authtoken (from https://dashboard.ngrok.com/get-started/your-authtoken):\")\n",
        "    ngrok_token = input()\n",
        "    ngrok.set_auth_token(ngrok_token)\n",
        "    public_url = ngrok.connect(5000).public_url\n",
        "    print(f\"Dashboard is publicly accessible at: {public_url}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}